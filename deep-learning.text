
                                    A Single neuron

Керас - це високорівневий інтерфейс для роботи з нейронними мережами в бібліотеці TensorFlow.
Він надає зручний спосіб визначення, тренування і оцінки нейронних мереж.

У вашому фрагменті коду використовуються імпорти з бібліотеки TensorFlow і її модулів Keras.
Основний функціонал для побудови моделей машинного навчання розміщений у модулі keras.models та keras.layers.

Наприклад, ви можете використовувати keras.models.Sequential для послідовної моделі, де шари можуть бути додані один за одним.
Шари визначаються за допомогою keras.layers, наприклад, Dense для повнозв'язного шару.


-----------------------------------------------------------------------------------------------

                                    Deep Neural Networks


Neural networks typically organize their neurons into layers.
When we collect together linear units having a common set of inputs we get a dense layer.

https://www.tensorflow.org/api_docs/python/tf/keras/layers

It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself.
Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear.
What we need are activation functions.

An activation function is simply some function we apply to each of a layer's outputs (its activations).
The most common is the rectifier function - max(0,x).

The rectifier function has a graph that's a line with the negative part "rectified" to zero.
Applying the function to the outputs of a neuron will put a bend in the data, moving us away from simple lines.

When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU.
(For this reason, it's common to call the rectifier function the "ReLU function".)
Applying a ReLU activation to a linear unit means the output becomes max(0, w * x + b).

The layers before the output layer are sometimes called hidden since we never see their outputs directly.

Now, notice that the final (output) layer is a linear unit (meaning, no activation function).
That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value.
Other tasks (like classification) might require an activation function on the output.


-----------------------------------------------------------------------------------------------

                                Stochastic Gradient Descent


A "loss function" that measures how good the network's predictions are.
An "optimizer" that can tell the network how to change its weights.

The loss function measures the disparity between the the target's true value and the value the model predicts.
A common loss function for regression problems is the mean absolute error or MAE.
For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).
Besides MAE, other loss functions you might see for regression problems are
the mean-squared error (MSE) or the Huber loss (both available in Keras).

We've described the problem we want the network to solve, but now we need to say how to solve it.
This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.
Virtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent.
They are iterative algorithms that train a network in steps. One step of training goes like this:
    1. Sample some training data and run it through the network to make predictions.
    2. Measure the loss between the predictions and the true values.
    3. Finally, adjust the weights in a direction that makes the loss smaller.
Then just do this over and over until the loss is as small as you like (or until it won't decrease any further).

Each iteration's sample of training data is called a minibatch (or often just "batch"),
while a complete round of the training data is called an epoch.
The number of epochs you train for is how many times the network will see each training example.

Smaller batch sizes gave noisier weight updates and loss curves.
This is because each batch is a small sample of data and smaller samples tend to give noisier estimates.
Smaller batches can have an "averaging" effect though which can be beneficial.

Smaller learning rates make the updates smaller and the training takes longer to converge.
Large learning rates can speed up training, but don't "settle in" to a minimum as well.
When the learning rate is too large, the training can fail completely.

Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way).
The size of these shifts is determined by the learning rate.
A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.

The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds.
Their interaction is often subtle and the right choice for these parameters isn't always obvious.

Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results.
Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning
(it is "self tuning", in a sense).
Adam is a great general-purpose optimizer.


-----------------------------------------------------------------------------------------------
                                Overfitting and Underfitting


This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise.
Underfitting the training set is when the loss is not as low as it could be because the model hasn't learned enough signal.
Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise.
The trick to training deep learning models is finding the best balance between the two.

The training data as being of two kinds: signal and noise.
The signal is the part that generalizes, the part that can help our model make predictions from new data.
The noise is that part that is only true of the training data;
the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental,
non-informative patterns that can't actually help the model make predictions.
The noise is the part might look useful but really isn't.

A model's capacity refers to the size and complexity of the patterns it is able to learn.
For neural networks, this will largely be determined by how many neurons it has and how they are connected together.
If it appears that your network is underfitting the data, you should try increasing its capacity.

You can increase the capacity of a network either by making it wider (more units to existing layers)
or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships,
while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.

We mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training.
To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore.
Interrupting the training this way is called early stopping.

In Keras, we include early stopping in our training through a callback.
A callback is just a function you want run every so often while the network trains.
The early stopping callback will run after every epoch. 

These parameters say: "If there hasn't been at least an improvement of 0.001 in the validation loss over the previous 20 epochs,
then stop the training and keep the best model you found."


-----------------------------------------------------------------------------------------------
                            Dropout and Batch Normalization


There are two kinds of special layers, not containing any neurons themselves,
but that add some functionality that can sometimes benefit a model in various ways.
Both are commonly used in modern architectures.

The first of these is the "dropout layer", which can help correct overfitting.
In the last lesson we talked about how overfitting is caused by the network learning spurious patterns in the training data.
To recognize these spurious patterns a network will often rely on very a specific combinations of weight,
a kind of "conspiracy" of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.

This is the idea behind dropout.
To break up these conspiracies, we randomly drop out some fraction of a layer's input units every step of training,
making it much harder for the network to learn those spurious patterns in the training data. Instead,
it has to search for broad, general patterns, whose weight patterns tend to be more robust.

You could also think about dropout as creating a kind of ensemble of networks.
The predictions will no longer be made by one big network, but instead by a committee of smaller networks.
Individuals in the committee tend to make different kinds of mistakes, but be right at the same time,
making the committee as a whole better than any individual. 


                            Batch Normalization
The next special layer we'll look at performs "batch normalization" (or "batchnorm"),
which can help correct training that is slow or unstable.

With neural networks, it's generally a good idea to put all of your data on a common scale,
perhaps with something like scikit-learn's StandardScaler or MinMaxScaler.
The reason is that SGD will shift the network weights in proportion to how large an activation the data produces.
Features that tend to produce activations of very different sizes can make for unstable training behavior.

In fact, we have a special kind of layer that can do this, the batch normalization layer.
A batch normalization layer looks at each batch as it comes in,
first normalizing the batch with its own mean and standard deviation,
and then also putting the data on a new scale with two trainable rescaling parameters.
Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.

Most often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance).
Models with batchnorm tend to need fewer epochs to complete training.
Moreover, batchnorm can also fix various problems that can cause the training to get "stuck".
Consider adding batch normalization to your models, especially if you're having trouble during training.