Керас - це високорівневий інтерфейс для роботи з нейронними мережами в бібліотеці TensorFlow.
Він надає зручний спосіб визначення, тренування і оцінки нейронних мереж.

У вашому фрагменті коду використовуються імпорти з бібліотеки TensorFlow і її модулів Keras.
Основний функціонал для побудови моделей машинного навчання розміщений у модулі keras.models та keras.layers.

Наприклад, ви можете використовувати keras.models.Sequential для послідовної моделі, де шари можуть бути додані один за одним.
Шари визначаються за допомогою keras.layers, наприклад, Dense для повнозв'язного шару.

Neural networks typically organize their neurons into layers.
When we collect together linear units having a common set of inputs we get a dense layer.

https://www.tensorflow.org/api_docs/python/tf/keras/layers

It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself.
Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear.
What we need are activation functions.

An activation function is simply some function we apply to each of a layer's outputs (its activations).
The most common is the rectifier function - max(0,x).

The rectifier function has a graph that's a line with the negative part "rectified" to zero.
Applying the function to the outputs of a neuron will put a bend in the data, moving us away from simple lines.

When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU.
(For this reason, it's common to call the rectifier function the "ReLU function".)
Applying a ReLU activation to a linear unit means the output becomes max(0, w * x + b).

The layers before the output layer are sometimes called hidden since we never see their outputs directly.

Now, notice that the final (output) layer is a linear unit (meaning, no activation function).
That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value.
Other tasks (like classification) might require an activation function on the output.